import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer 
from sklearn.pipeline import Pipeline as SklearnPipeline
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
import os 
import sys 
from feature_engineering import FeatureEngineer
from text_cleaner import TextCleaner 
from vectorizers import Word2VecTransformer
project_root = os.path.dirname(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)



def create_preprocessing_pipeline(vectorizer_type='tfidf'):
    """
    H√†m n√†y ch·ªâ tr·∫£ v·ªÅ Pipeline x·ª≠ l√Ω d·ªØ li·ªáu: 
    Feature Eng -> Clean -> Vectorize.
    
    KH√îNG C√ì SMOTE (C√¢n b·∫±ng d·ªØ li·ªáu).
    KH√îNG C√ì MODEL (XGBoost).
    """
    
    # 1. ƒê·ªãnh nghƒ©a c·ªôt
    num_cols = ['telecommuting', 'has_company_logo', 'has_questions']
    text_cols = ['title', 'location', 'department', 'company_profile', 'description', 
                'requirements', 'benefits', 'employment_type', 'required_experience', 
                'required_education', 'industry', 'function']
    
    # 2. Ch·ªçn Vectorizer
    if vectorizer_type == 'count':
        vec_step = CountVectorizer(max_features=5000)
    elif vectorizer_type == 'tfidf':
        vec_step = TfidfVectorizer(max_features=5000)
    elif vectorizer_type == 'word2vec':
        vec_step = Word2VecTransformer(vector_size=100, window=5, min_count=2)
    
    # 3. Preprocessor (ColumnTransformer)
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', SimpleImputer(strategy='median'), num_cols),
            ('text', SklearnPipeline([
                ('cleaner', TextCleaner()), 
                ('vec', vec_step)
            ]), text_cols),
            ('flags', 'passthrough', ['key_note'])
        ],
        remainder='drop'
    )
    
    # 4. C√°c b∆∞·ªõc Pipeline
    steps = [
        ('engineer', FeatureEngineer()), 
        ('preprocessor', preprocessor)
    ]
    
    # === ƒê√É X√ìA PH·∫¶N 5 (SMOTE) ===
    # Pipeline n√†y gi·ªù ch·ªâ bi·∫øn ƒë·ªïi d·ªØ li·ªáu th√¥ th√†nh s·ªë
    
    return ImbPipeline(steps=steps)

path_data = os.path.join(project_root,"data","data_train.csv")
df = pd.read_csv(path_data)
X_train= df.drop("fraudulent",axis= 1)
y_train = df['fraudulent']


# --- 1. C·∫§U H√åNH C√ÅC PH∆Ø∆†NG √ÅN TI·ªÄN X·ª¨ L√ù ---
# (L∆∞u √Ω: T√™n file m√¨nh ƒë·ªïi ti·ªÅn t·ªë th√†nh 'Preprocess_' cho d·ªÖ ph√¢n bi·ªát)
vectorizer_types = ["tfidf", "count", "word2vec"]


print("\nüöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y TI·ªÄN X·ª¨ L√ù & L∆ØU D·ªÆ LI·ªÜU S·∫†CH...\n")

# --- 3. V√íNG L·∫∂P X·ª¨ L√ù ---
for vec_type in vectorizer_types:
    # ƒê·∫∑t t√™n file t·ª± ƒë·ªông
    name = f"Preprocess_{vec_type}" 
    print(f"‚è≥ ƒêang x·ª≠ l√Ω: {name}...")
    
    # A. G·ªçi h√†m t·∫°o Pipeline (Kh√¥ng truy·ªÅn imbalance_strategy n·ªØa)
    preprocessor = create_preprocessing_pipeline(vectorizer_type=vec_type)
    
    # B. Fit & Transform (Thay v√¨ fit_resample)
    # H√†m n√†y ch·ªâ h·ªçc t·ª´ v·ª±ng v√† bi·∫øn ƒë·ªïi th√†nh s·ªë. KH√îNG sinh th√™m d·ªØ li·ªáu.
    X_processed = preprocessor.fit_transform(X_train, y_train)
    
    # C. L∆∞u k·∫øt qu·∫£
    
    # 1. L∆∞u Pipeline (Ch·ª©a logic x·ª≠ l√Ω)
    pipe_path = os.path.join(project_root,"models", f"{name}_pipeline.pkl")
    joblib.dump(preprocessor, pipe_path)
    
    # 2. L∆∞u D·ªØ li·ªáu s·∫°ch
    # L∆∞u √Ω: y_train gi·ªØ nguy√™n v√¨ ta kh√¥ng SMOTE
    data_path = os.path.join(project_root,"models", f"{name}_data.pkl")
    joblib.dump((X_processed, y_train), data_path)
    
    print(f"   ‚úÖ Pipeline l∆∞u t·∫°i: {pipe_path}")
    print(f"   ‚úÖ D·ªØ li·ªáu s·∫°ch ({X_processed.shape}) l∆∞u t·∫°i: {data_path}\n")

print("üéâ HO√ÄN T·∫§T! B·∫°n ƒë√£ c√≥ b·ªô d·ªØ li·ªáu s·∫°ch (ch∆∞a c√¢n b·∫±ng).")


project_root,"fraud-detection-post","models"