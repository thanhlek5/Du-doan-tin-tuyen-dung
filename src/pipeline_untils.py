import pandas as pd
import numpy as np
import re
import nltk
import spacy
from gensim.models import Word2Vec
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer 
from sklearn.pipeline import Pipeline as SklearnPipeline
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
import os 
import sys 
project_root = os.path.abspath(os.path.join(os.getcwd(),".."))
if project_root not in sys.path:
    sys.path.append(project_root)
from preprocessor_utils import split_data    


class TextCleaner(BaseEstimator, TransformerMixin):
    def __init__(self):
        # --- 1. MUA GIA V·ªä V√Ä CH·∫¢O (Kh·ªüi t·∫°o t√†i nguy√™n ngay trong Class) ---
        print("Dataset initialization: Loading NLP resources...")
        
        # T·∫£i NLTK Stopwords (T·ª± ƒë·ªông t·∫£i n·∫øu ch∆∞a c√≥)
        try:
            self.stopwords = set(nltk.corpus.stopwords.words('english'))
        except LookupError:
            nltk.download('stopwords')
            nltk.download('punkt')
            self.stopwords = set(nltk.corpus.stopwords.words('english'))
            
        # T·∫£i SpaCy Model (T·ª± ƒë·ªông t·∫£i n·∫øu ch∆∞a c√≥)
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("Downloading language model for the spaCy POS tagger\n"
                "(don't worry, this will only happen once)")
            from spacy.cli import download
            download("en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")
            
    def fit(self, X, y=None):
        return self
    # --- 2. C√îNG TH·ª®C N·∫§U ƒÇN (Chuy·ªÉn h√†m v√†o trong class) ---
    def _internal_clear_text(self, text: str) -> str:
        """H√†m l√†m s·∫°ch d√πng t√†i nguy√™n n·ªôi b·ªô (self.stopwords)"""
        if not isinstance(text, str):
            return ""
        whitelist = {'show', 'unless', 'me', 'anywhere', 'he', 'again', 'from', 'my', 'may', 'before', 'full', 'name', 'done', 'nothing', 'others', 'per', 'above', 'below', 'six', 'your', 'down', 'own', 'hence', 'thereby', 'within', 'call', 'ours', 'third', 'must', 'off', 'say', 'ten', 'eight', 'his', 'should', 'serious', 'any', 'otherwise', 'mostly', 'much', 'several', 'under', 'no', 'amount', 'toward', 'amongst', 'via', 'mine', 'hundred', 'whose'}
        final_stopwords = self.stopwords - whitelist 
        # Logic l√†m s·∫°ch c≈© c·ªßa b·∫°n
        text = text.lower()
        text = re.sub(r'<.*?>', "", text)
        text = re.sub(r'http\S+', '', text)
        text = re.sub(r'[^a-zA-Z0-9]', ' ', text)
        
        tokens = nltk.word_tokenize(text)
        # QUAN TR·ªåNG: D√πng self.stopwords thay v√¨ bi·∫øn to√†n c·ª•c
        tokens = [word for word in tokens if word not in final_stopwords]
        return " ".join(tokens)

    def _internal_normalize_text(self, text: str) -> str:
        """H√†m chu·∫©n h√≥a d√πng t√†i nguy√™n n·ªôi b·ªô (self.nlp)"""
        # QUAN TR·ªåNG: D√πng self.nlp thay v√¨ bi·∫øn to√†n c·ª•c
        doc = self.nlp(text)
        normalized_words = [token.lemma_ for token in doc]
        return ' '.join(normalized_words)

    def transform(self, X):
        # X l√† DataFrame
        X_filled = X.fillna("missing")
        
        # G·ªôp c·ªôt
        combined = X_filled.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
        
        # G·ªçi h√†m n·ªôi b·ªô (d√πng self.)
        # B∆∞·ªõc 1: Clear text
        cleaned = combined.apply(self._internal_clear_text)
        
        # B∆∞·ªõc 2: Normalize
        normalized = cleaned.apply(self._internal_normalize_text)
        
        return normalized

# --- ƒê·ªäNH NGHƒ®A CLASS WORD2VEC TRANSFORMER ---
class Word2VecTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, vector_size=100, window=5, min_count=1):
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.model = None

    def fit(self, X, y=None):
        """
        Hu·∫•n luy·ªán m√¥ h√¨nh Word2Vec tr√™n t·∫≠p d·ªØ li·ªáu.
        X: List ho·∫∑c Series c√°c chu·ªói vƒÉn b·∫£n (ƒë√£ ƒë∆∞·ª£c clean).
        """
        # T√°ch t·ª´ (Tokenize) ƒë∆°n gi·∫£n b·∫±ng split()
        # L∆∞u √Ω: X n√™n l√† vƒÉn b·∫£n ƒë√£ qua b∆∞·ªõc TextCleaner (s·∫°ch s·∫Ω)
        sentences = [str(text).split() for text in X]
        
        # Train m√¥ h√¨nh Gensim Word2Vec
        self.model = Word2Vec(sentences, 
                            vector_size=self.vector_size, 
                            window=self.window, 
                            min_count=self.min_count, 
                            workers=4)
        return self

    def transform(self, X):
        """
        Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh Vector trung b√¨nh (Average Word Vector).
        """
        # H√†m con ƒë·ªÉ t√≠nh vector trung b√¨nh cho 1 c√¢u
        def get_avg_vector(text):
            if self.model is None:
                return np.zeros(self.vector_size)
                
            words = str(text).split()
            # L·∫•y vector c·ªßa c√°c t·ª´ c√≥ trong t·ª´ ƒëi·ªÉn c·ªßa model
            word_vectors = [self.model.wv[w] for w in words if w in self.model.wv]
            
            # N·∫øu c√¢u kh√¥ng c√≥ t·ª´ n√†o trong t·ª´ ƒëi·ªÉn (ho·∫∑c r·ªóng) -> Tr·∫£ v·ªÅ vector 0
            if len(word_vectors) == 0:
                return np.zeros(self.vector_size)
            
            # T√≠nh trung b√¨nh c·ªông c√°c vector
            return np.mean(word_vectors, axis=0)
            
        # √Åp d·ª•ng cho to√†n b·ªô d·ªØ li·ªáu X
        # np.vstack gi√∫p x·∫øp ch·ªìng c√°c vector th√†nh ma tr·∫≠n
        return np.vstack([get_avg_vector(text) for text in X])
    
class FeatureEngineer(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X = X.copy() # Copy ra ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng d·ªØ li·ªáu g·ªëc
        
        # T·∫°o bi·∫øn t·∫°m g·ªôp text ƒë·ªÉ t√¨m ki·∫øm cho nhanh
        # (Ch·ªâ d√πng n·ªôi b·ªô ƒë·ªÉ t√¨m features, kh√¥ng ghi ƒë√® c·ªôt description g·ªëc)
        temp_text = X['description'].fillna('') + " " + X['requirements'].fillna('')
        
        # 1. T·∫†O C·ªòT 'chain' (Ph√°t hi·ªán m√£ r√°c bot spam)
        garbage_char = '0fa3f7c5e23a16de16a841e368006cae916884407d90b154dfef3976483a71ae'
        X['chain'] = temp_text.apply(lambda x: 1 if garbage_char in str(x) else 0)
        
        # 2. T·∫†O C·ªòT 'key_note' (T·ª´ kh√≥a m·∫°o danh D·∫ßu kh√≠)
        keys = ['aker', 'subsea', 'action', 'novation']
        pattern = '|'.join(keys) # T·∫°o regex: "aker|subsea|action..."
        X['key_note'] = temp_text.str.contains(pattern, case=False, na=False).astype(int)
        
        return X

def create_preprocessing_pipeline(vectorizer_type='tfidf'):
    """
    H√†m n√†y ch·ªâ tr·∫£ v·ªÅ Pipeline x·ª≠ l√Ω d·ªØ li·ªáu: 
    Feature Eng -> Clean -> Vectorize.
    
    KH√îNG C√ì SMOTE (C√¢n b·∫±ng d·ªØ li·ªáu).
    KH√îNG C√ì MODEL (XGBoost).
    """
    
    # 1. ƒê·ªãnh nghƒ©a c·ªôt
    num_cols = ['telecommuting', 'has_company_logo', 'has_questions']
    text_cols = ['title', 'location', 'department', 'company_profile', 'description', 
                'requirements', 'benefits', 'employment_type', 'required_experience', 
                'required_education', 'industry', 'function']
    
    # 2. Ch·ªçn Vectorizer
    if vectorizer_type == 'count':
        vec_step = CountVectorizer(max_features=5000)
    elif vectorizer_type == 'tfidf':
        vec_step = TfidfVectorizer(max_features=5000)
    elif vectorizer_type == 'word2vec':
        vec_step = Word2VecTransformer(vector_size=100, window=5, min_count=2)
    
    # 3. Preprocessor (ColumnTransformer)
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', SimpleImputer(strategy='median'), num_cols),
            ('text', SklearnPipeline([
                ('cleaner', TextCleaner()), 
                ('vec', vec_step)
            ]), text_cols),
            ('flags', 'passthrough', ['key_note', 'chain'])
        ],
        remainder='drop'
    )
    
    # 4. C√°c b∆∞·ªõc Pipeline
    steps = [
        ('engineer', FeatureEngineer()), 
        ('preprocessor', preprocessor)
    ]
    
    # === ƒê√É X√ìA PH·∫¶N 5 (SMOTE) ===
    # Pipeline n√†y gi·ªù ch·ªâ bi·∫øn ƒë·ªïi d·ªØ li·ªáu th√¥ th√†nh s·ªë
    
    return ImbPipeline(steps=steps)

path_data = os.path.join(project_root,"fraud-detection-post","data","data_train.csv")
df = pd.read_csv(path_data)
X_train,y_train = split_data(df)

# --- 1. C·∫§U H√åNH C√ÅC PH∆Ø∆†NG √ÅN TI·ªÄN X·ª¨ L√ù ---
# (L∆∞u √Ω: T√™n file m√¨nh ƒë·ªïi ti·ªÅn t·ªë th√†nh 'Preprocess_' cho d·ªÖ ph√¢n bi·ªát)
vectorizer_types = ["tfidf", "count", "word2vec"]


print("\nüöÄ B·∫ÆT ƒê·∫¶U CH·∫†Y TI·ªÄN X·ª¨ L√ù & L∆ØU D·ªÆ LI·ªÜU S·∫†CH...\n")

# --- 3. V√íNG L·∫∂P X·ª¨ L√ù ---
for vec_type in vectorizer_types:
    # ƒê·∫∑t t√™n file t·ª± ƒë·ªông
    name = f"Preprocess_{vec_type}" 
    print(f"‚è≥ ƒêang x·ª≠ l√Ω: {name}...")
    
    # A. G·ªçi h√†m t·∫°o Pipeline (Kh√¥ng truy·ªÅn imbalance_strategy n·ªØa)
    preprocessor = create_preprocessing_pipeline(vectorizer_type=vec_type)
    
    # B. Fit & Transform (Thay v√¨ fit_resample)
    # H√†m n√†y ch·ªâ h·ªçc t·ª´ v·ª±ng v√† bi·∫øn ƒë·ªïi th√†nh s·ªë. KH√îNG sinh th√™m d·ªØ li·ªáu.
    X_processed = preprocessor.fit_transform(X_train, y_train)
    
    # C. L∆∞u k·∫øt qu·∫£
    
    # 1. L∆∞u Pipeline (Ch·ª©a logic x·ª≠ l√Ω)
    pipe_path = os.path.join(project_root,"fraud-detection-post","models", f"{name}_pipeline.pkl")
    joblib.dump(preprocessor, pipe_path)
    
    # 2. L∆∞u D·ªØ li·ªáu s·∫°ch
    # L∆∞u √Ω: y_train gi·ªØ nguy√™n v√¨ ta kh√¥ng SMOTE
    data_path = os.path.join(project_root,"fraud-detection-post","models", f"{name}_data.pkl")
    joblib.dump((X_processed, y_train), data_path)
    
    print(f"   ‚úÖ Pipeline l∆∞u t·∫°i: {pipe_path}")
    print(f"   ‚úÖ D·ªØ li·ªáu s·∫°ch ({X_processed.shape}) l∆∞u t·∫°i: {data_path}\n")

print("üéâ HO√ÄN T·∫§T! B·∫°n ƒë√£ c√≥ b·ªô d·ªØ li·ªáu s·∫°ch (ch∆∞a c√¢n b·∫±ng).")


project_root,"fraud-detection-post","models"